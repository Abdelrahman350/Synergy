Epoch 1/100:
2021-12-02 20:39:24.258963: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100
2021-12-02 20:39:25.000737: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
























































































































































































































































































1633/1633 [==============================] - 572s 349ms/step - train_loss: 1988.5795





























273/279 [============================>.] - ETA: 1s - valid_loss: 1876.8236
279/279 [==============================] - 60s 215ms/step - valid_loss: 1877.1052 - train_loss: 1988.5793 - val_loss: 1877.1052
The validation loss improved from inf to 1877.105224609375.                 Saving model Model.h5































































































































































































































































































1633/1633 [==============================] - 577s 353ms/step - train_loss: 1885.9509





























271/279 [============================>.] - ETA: 1s - valid_loss: 1877.7620
279/279 [==============================] - 60s 214ms/step - valid_loss: 1877.2974 - train_loss: 1885.9508 - val_loss: 1877.2974































































































































































































































































































1633/1633 [==============================] - 577s 353ms/step - train_loss: 1886.0732





























279/279 [==============================] - 60s 214ms/step - valid_loss: 1876.6750 - train_loss: 1886.0734 - val_loss: 1876.6750
The validation loss improved from 1877.105224609375 to 1876.675048828125.                 Saving model Model.h5
Epoch 4/100:































































































































































































































































































1633/1633 [==============================] - 578s 354ms/step - train_loss: 1885.8107





























275/279 [============================>.] - ETA: 1s - valid_loss: 1876.6693
279/279 [==============================] - 60s 214ms/step - valid_loss: 1877.1771 - train_loss: 1885.8107 - val_loss: 1877.1771
































































































































































































































































































1633/1633 [==============================] - 578s 354ms/step - train_loss: 1885.8538





























279/279 [==============================] - 60s 214ms/step - valid_loss: 1877.0068 - train_loss: 1885.8538 - val_loss: 1877.0068
Epoch 6/100:































































































































































































































































































1633/1633 [==============================] - 579s 354ms/step - train_loss: 1885.9310





























279/279 [==============================] - 60s 214ms/step - valid_loss: 1877.1437 - train_loss: 1885.9309 - val_loss: 1877.1437
   0/1633 [..............................] - ETA: 0s - train_loss: 1775.6052
































































































































































































































































































1633/1633 [==============================] - 579s 354ms/step - train_loss: 1885.8934





























279/279 [==============================] - 60s 214ms/step - valid_loss: 1876.7849 - train_loss: 1885.8934 - val_loss: 1876.7849
   1/1633 [..............................] - ETA: 17:13 - train_loss: 1871.1677
































































































































































































































































































1633/1633 [==============================] - 579s 355ms/step - train_loss: 1885.8804





























279/279 [==============================] - 60s 214ms/step - valid_loss: 1877.4193 - train_loss: 1885.8805 - val_loss: 1877.4193
   2/1633 [..............................] - ETA: 9:37 - train_loss: 1821.9937
































































































































































































































































































1633/1633 [==============================] - 579s 355ms/step - train_loss: 1885.9728





























279/279 [==============================] - 60s 214ms/step - valid_loss: 1876.8263 - train_loss: 1885.9728 - val_loss: 1876.8263
   0/1633 [..............................] - ETA: 0s - train_loss: 1772.3721

































































































































































































































































































1633/1633 [==============================] - 580s 355ms/step - train_loss: 1886.1757





























279/279 [==============================] - 60s 215ms/step - valid_loss: 1877.0861 - train_loss: 1886.1755 - val_loss: 1877.0861
   3/1633 [..............................] - ETA: 9:37 - train_loss: 1851.2332
































































































































































































































































































1633/1633 [==============================] - 580s 355ms/step - train_loss: 1886.0458





























274/279 [============================>.] - ETA: 1s - valid_loss: 1876.5604
279/279 [==============================] - 60s 215ms/step - valid_loss: 1877.1083 - train_loss: 1886.0458 - val_loss: 1877.1083
































































































































































































































































































1633/1633 [==============================] - 579s 355ms/step - train_loss: 1885.8998





























272/279 [============================>.] - ETA: 1s - valid_loss: 1877.2257
279/279 [==============================] - 60s 215ms/step - valid_loss: 1877.1854 - train_loss: 1885.8999 - val_loss: 1877.1854

































































































































































































































































































1633/1633 [==============================] - 581s 355ms/step - train_loss: 1885.9747




























279/279 [==============================] - 60s 215ms/step - valid_loss: 1876.7238 - train_loss: 1885.9747 - val_loss: 1876.7238
   0/1633 [..............................] - ETA: 0s - train_loss: 1778.3706

































































































































































































































































































1633/1633 [==============================] - 580s 355ms/step - train_loss: 1885.9845





























275/279 [============================>.] - ETA: 1s - valid_loss: 1876.3761
279/279 [==============================] - 60s 215ms/step - valid_loss: 1876.8037 - train_loss: 1885.9845 - val_loss: 1876.8037
































































































































































































































































































1633/1633 [==============================] - 579s 354ms/step - train_loss: 1885.9569





























277/279 [============================>.] - ETA: 0s - valid_loss: 1876.8961
279/279 [==============================] - 60s 214ms/step - valid_loss: 1877.0090 - train_loss: 1885.9568 - val_loss: 1877.0090



















































































 471/1633 [=======>......................] - ETA: 6:53 - train_loss: 1889.4419
Traceback (most recent call last):
  File "train.py", line 40, in <module>
    train(model, training_data_generator, validation_data_generator, 100,\
  File "/media/avidbeam/workspace/Abdelrahman_Workspace/Project_Codes/Synergy/utils/custom_fit.py", line 22, in train
    train_loss = train_batch(X_batch,\
  File "/media/avidbeam/workspace/Abdelrahman_Workspace/Project_Codes/Synergy/utils/custom_fit.py", line 59, in train_batch
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py", line 672, in apply_gradients
    return self._distributed_apply(strategy, grads_and_vars, name,
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py", line 721, in _distributed_apply
    update_op = distribution.extended.update(
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2634, in update
    return self._update(var, fn, args, kwargs, group)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 3709, in _update
    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 3719, in _update_non_slot
    return nest.map_structure(self._local_results, result)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/util/nest.py", line 868, in map_structure
    return pack_sequence_as(
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/util/nest.py", line 757, in pack_sequence_as
    return _pack_sequence_as(structure, flat_sequence, expand_composites)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/util/nest.py", line 615, in _pack_sequence_as
    if not is_seq(flat_sequence):

 472/1633 [=======>......................] - ETA: 6:53 - train_loss: 1889.4016