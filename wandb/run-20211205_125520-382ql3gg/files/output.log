Epoch 1/500:
2021-12-05 12:55:23.119412: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100
2021-12-05 12:55:23.860365: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
























































































































































































































































































































































































2654/3266 [=======================>......] - ETA: 2:54 - train_loss: 1190.8676
Traceback (most recent call last):
  File "train.py", line 40, in <module>
    train(model, training_data_generator, validation_data_generator, 500,\
  File "/media/avidbeam/workspace/Abdelrahman_Workspace/Project_Codes/Synergy/utils/custom_fit.py", line 32, in train
    train_loss = train_batch(X_batch,\
  File "/media/avidbeam/workspace/Abdelrahman_Workspace/Project_Codes/Synergy/utils/custom_fit.py", line 69, in train_batch
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py", line 672, in apply_gradients
    return self._distributed_apply(strategy, grads_and_vars, name,
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py", line 718, in _distributed_apply
    with name_scope_only_in_function_or_graph(
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py", line 92, in name_scope_only_in_function_or_graph
    return NullContextmanager()
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py", line 67, in __init__
    def __init__(self, *args, **kwargs):

2659/3266 [=======================>......] - ETA: 2:52 - train_loss: 1189.6515