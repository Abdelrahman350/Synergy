Epoch 1/100:
2021-12-02 17:30:52.954057: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100
2021-12-02 17:30:53.691375: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.




































































































































































































































































































































3266/3266 [==============================] - 653s 200ms/step - train_loss: 1918.8577






























559/559 [==============================] - 63s 113ms/step - valid_loss: 1874.6655 - train_loss: 1918.8577 - val_loss: 1874.6655
The validation loss improved from inf to 1874.66552734375.                 Saving model Model.h5
Epoch 2/100:





































































































































































































































































































































3266/3266 [==============================] - 655s 200ms/step - train_loss: 1881.8864































552/559 [============================>.] - ETA: 0s - valid_loss: 1873.7808
559/559 [==============================] - 63s 113ms/step - valid_loss: 1874.3781 - train_loss: 1881.8862 - val_loss: 1874.3781
The validation loss improved from 1874.66552734375 to 1874.3780517578125.                 Saving model Model.h5







































































































































































































































































































































3266/3266 [==============================] - 657s 201ms/step - train_loss: 1881.9181






























544/559 [============================>.] - ETA: 1s - valid_loss: 1874.7755
559/559 [==============================] - 64s 114ms/step - valid_loss: 1874.8092 - train_loss: 1881.9181 - val_loss: 1874.8093







































































































































































































































































































































3266/3266 [==============================] - 656s 201ms/step - train_loss: 1881.9332































557/559 [============================>.] - ETA: 0s - valid_loss: 1874.5297
559/559 [==============================] - 64s 113ms/step - valid_loss: 1874.6273 - train_loss: 1881.9332 - val_loss: 1874.6273






































































































































































































































































































































3266/3266 [==============================] - 656s 201ms/step - train_loss: 1881.9720































550/559 [============================>.] - ETA: 1s - valid_loss: 1873.7460
559/559 [==============================] - 64s 113ms/step - valid_loss: 1874.3259 - train_loss: 1881.9719 - val_loss: 1874.3259
The validation loss improved from 1874.3780517578125 to 1874.325927734375.                 Saving model Model.h5







































































































































































































































































































































3266/3266 [==============================] - 658s 201ms/step - train_loss: 1881.8552































549/559 [============================>.] - ETA: 1s - valid_loss: 1873.7963
559/559 [==============================] - 64s 113ms/step - valid_loss: 1874.4270 - train_loss: 1881.8553 - val_loss: 1874.4270







































































































































































































































































































































3266/3266 [==============================] - 657s 201ms/step - train_loss: 1881.9233































553/559 [============================>.] - ETA: 0s - valid_loss: 1874.2042
559/559 [==============================] - 64s 113ms/step - valid_loss: 1874.6238 - train_loss: 1881.9233 - val_loss: 1874.6238






































































































































































































































































































































3266/3266 [==============================] - 656s 201ms/step - train_loss: 1881.9771































559/559 [==============================] - 64s 113ms/step - valid_loss: 1874.8313 - train_loss: 1881.9771 - val_loss: 1874.8313
Epoch 9/100:


























































































































































































































2161/3266 [==================>...........] - ETA: 3:43 - train_loss: 1882.7249
Traceback (most recent call last):
  File "train.py", line 40, in <module>
    train(model, training_data_generator, validation_data_generator, 100,\
  File "/media/avidbeam/workspace/Abdelrahman_Workspace/Project_Codes/Synergy/utils/custom_fit.py", line 22, in train
    train_loss = train_batch(X_batch,\
  File "/media/avidbeam/workspace/Abdelrahman_Workspace/Project_Codes/Synergy/utils/custom_fit.py", line 58, in train_batch
    grads = tape.gradient(loss_value, model.trainable_weights)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py", line 1084, in gradient
    flat_grad = imperative_grad.imperative_grad(
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py", line 71, in imperative_grad
    return pywrap_tfe.TFE_Py_TapeGradient(
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py", line 159, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py", line 270, in _MeanGrad
    return math_ops.truediv(sum_grad, math_ops.cast(factor, sum_grad.dtype)), None
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 1096, in op_dispatch_handler
    return dispatch_target(*args, **kwargs)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py", line 1566, in truediv
    return _truediv_python3(x, y, name)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py", line 1504, in _truediv_python3
    return gen_math_ops.real_div(x, y, name=name)
  File "/home/avidbeam/.virtualenvs/TensorFlow_2/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py", line 7871, in real_div
    _result = pywrap_tfe.TFE_Py_FastPathExecute(
KeyboardInterrupt